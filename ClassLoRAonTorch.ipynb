{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34d718ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aab8f173",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, original_layer, rank=8, alpha=16):\n",
    "        super().__init__()\n",
    "        self.base_layer = original_layer\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # 冻结原始注意力层的参数\n",
    "        for param in self.base_layer.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        in_dim = self.base_layer.weight.shape[1]\n",
    "        out_dim = self.base_layer.weight.shape[0]\n",
    "        base_dtype = self.base_layer.weight.dtype\n",
    "        base_device = self.base_layer.weight.device\n",
    "\n",
    "        # 明确指定 dtype 和 device\n",
    "        self.lora_A = nn.Linear(in_dim, rank, bias=False).to(dtype=base_dtype, device=base_device)\n",
    "        self.lora_B = nn.Linear(rank, out_dim, bias=False).to(dtype=base_dtype, device=base_device)\n",
    "        \n",
    "        # 初始化权重\n",
    "        nn.init.kaiming_uniform_(self.lora_A.weight, a=np.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 原始前向传播\n",
    "        out = self.base_layer(x)\n",
    "        # LoRA部分\n",
    "        lora_out = self.lora_B(self.lora_A(x))\n",
    "        # 缩放并合并\n",
    "        return out + (self.alpha / self.rank) * lora_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77d22813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c1f5292add480b9f95999a818e530a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef58bca25ab46b0a888e23b200e19e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "927a873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear_with_lora(model, target_name, rank=8, alpha=16):\n",
    "    for name, module in model.named_children():\n",
    "        if len(list(module.children())) > 0:  # 如果模块还有子模块，递归进入\n",
    "            replace_linear_with_lora(module, target_name, rank, alpha)\n",
    "        \n",
    "        if isinstance(module, nn.Linear) and name == target_name:\n",
    "            # 替换目标 Linear 层为 LoRALinear\n",
    "            lora_layer = LoRALinear(module, rank=rank, alpha=alpha)\n",
    "            setattr(model, name, lora_layer)\n",
    "\n",
    "def apply_lora_model(model,target_modules,rank=8,alpha=16):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    for target_name in target_modules:\n",
    "        replace_linear_with_lora(model,target_name,rank,alpha)\n",
    "        \n",
    "        # 计算可训练参数数量\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    # 计算总参数数量\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    # 计算可训练参数占比（百分比）\n",
    "    trainable_percent = (trainable_params / total_params) * 100\n",
    "    \n",
    "    print(\n",
    "        f\"trainable params: {trainable_params:,} || \"\n",
    "        f\"all params: {total_params:,} || \"\n",
    "        f\"trainable%: {trainable_percent:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d2dda7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 688,128 || all params: 596,738,048 || trainable%: 0.1153\n"
     ]
    }
   ],
   "source": [
    "apply_lora_model(model,[\"q_proj\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2367467c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen3ForCausalLM(\n",
      "  (model): Qwen3Model(\n",
      "    (embed_tokens): Embedding(151936, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen3DecoderLayer(\n",
      "        (self_attn): Qwen3Attention(\n",
      "          (q_proj): LoRALinear(\n",
      "            (base_layer): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "            (lora_A): Linear(in_features=1024, out_features=8, bias=False)\n",
      "            (lora_B): Linear(in_features=8, out_features=2048, bias=False)\n",
      "          )\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "        )\n",
      "        (mlp): Qwen3MLP(\n",
      "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
      "    (rotary_emb): Qwen3RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0edca33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(model.model.layers[0].self_attn.q_proj.lora_A.weight[0][0].requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7447b1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7746f660d49d4ca0854de2ffcff46fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e1a817f0a549a6ae3c4d8b00f5b534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50a7cb84da34bee805221e7349a4fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4591b0ab6b6c4a57bb9ce33fae66e938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89484d94b77944b1a8a84ae8286519eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91284a7c80d64177936c0eefe269bcbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04bd3164067948869badc80935430eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 加载GSM8K数据集\n",
    "dataset = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "\n",
    "# 预处理函数 - 根据你的任务需求调整\n",
    "def preprocess_function(examples):\n",
    "    # 这里以数学问题解答为例，构建输入输出格式\n",
    "    inputs = [f\"Question: {q}\\nAnswer:\" for q in examples[\"question\"]]\n",
    "    outputs = examples[\"answer\"]\n",
    "    return {\"input\": inputs, \"output\": outputs}\n",
    "\n",
    "# 预处理数据集\n",
    "dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a97482fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72', 'input': 'Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\\nAnswer:', 'output': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "579431d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d669ed8f1b49998fc3e8dc0dfeb220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ae69557acc411998d664968cfc345d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# 数据预处理函数\n",
    "def tokenize_function(examples):\n",
    "    # 将输入和输出拼接起来\n",
    "    texts = [inp + \" \" + out for inp, out in zip(examples[\"input\"], examples[\"output\"])]\n",
    "    # 对文本进行tokenize\n",
    "    tokenized = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=256)\n",
    "    # 创建labels - 将input部分的token设为-100，模型不会计算这些位置的loss\n",
    "    input_ids = tokenized[\"input_ids\"]\n",
    "    labels = []\n",
    "    for i in range(len(input_ids)):\n",
    "        input_len = len(tokenizer(examples[\"input\"][i], truncation=True, max_length=256)[\"input_ids\"])\n",
    "        labels.append([-100] * input_len + input_ids[i][input_len:])\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "# 对数据集进行tokenize\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15be7ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen-mylora-gsm8k\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    logging_steps=100,\n",
    "    fp16=True,\n",
    "    report_to=None,\n",
    "    push_to_hub=False,  \n",
    ")\n",
    "\n",
    "# 创建Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05224805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='5607' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/5607 : < :, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 训练模型\n",
    "trainer.train()\n",
    "\n",
    "# 保存模型\n",
    "model.save_pretrained(\"qwen-mylora-gsm8k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a27894d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 688,128 || all params: 596,738,048 || trainable%: 0.1153\n"
     ]
    }
   ],
   "source": [
    "# 1. 先加载原始模型\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cpu\"\n",
    ")\n",
    "\n",
    "# 2. 用你的LoRA类替换所有q_proj层\n",
    "apply_lora_model(model,[\"q_proj\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7442c8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\A'\n",
      "D:\\Temp\\ipykernel_6660\\4285516475.py:3: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  state_dict = load_file(\"D:\\AI\\ML\\MLSys Final\\LORA_Reconstruct\\qwen-mylora-gsm8k\\checkpoint-5607\\model.safetensors\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['lm_head.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "# 如果是Trainer保存的checkpoint，权重在 pytorch_model.bin 里\n",
    "state_dict = load_file(\"D:\\AI\\ML\\MLSys Final\\LORA_Reconstruct\\qwen-mylora-gsm8k\\checkpoint-5607\\model.safetensors\")\n",
    "model.load_state_dict(state_dict, strict=False)# strict=False可以防止小量不匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3896c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: If a book costs $15 and the sales tax is 8%, what is the total cost?\n",
      "Answer: The sales tax is 15 x 8/100 = $<<15*8/100=1.2>>1.2\n",
      "The total cost is 15 + 1.2 = $<<15+1.2=16.2>>16.2\n",
      "#### 16.2\n"
     ]
    }
   ],
   "source": [
    "def generate_answer(question):\n",
    "    input_text = f\"Question: {question}\\nAnswer:\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 测试\n",
    "question = \"If a book costs $15 and the sales tax is 8%, what is the total cost?\"\n",
    "print(generate_answer(question))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
